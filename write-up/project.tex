%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2014 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amssymb,amsthm,amsmath}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage[accepted]{icml2014} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2014}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Project Proposal for CS6780}

\begin{document} 

\twocolumn[
\icmltitle{Approximation Algorithms for Partitional and Hierarchical Clustering}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2014
% package.
\icmlauthor{Alice Paul (ajp336), Calvin Wylie (cjw278), David Lingenbrink (dal299)}{}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]


\section{Introduction}
As the data we have access to grows in variety and size and becomes cheaper to store, we need methods to efficiently learn from this data. 
Partitioning data (clustering) into similar groups (clusters) is one of the key tools to learn about a set of unlabeled data such as a set of customers who have visited our website or identifying handwritten words.
Clustering in these situations helps us learn underlying structure and classifications of our data, and it presents large data sets in a simplified and compressed view \cite{Jain}.

There are two main branches of clustering: partitional (or flat) clustering and hierarchical clustering. In partitional clustering we want to partition the data into disjoint sets whereas in hierarchical clustering we are interested in finding a nested tree of partitions. Often these problems are solved using heuristic algorithms which have no guarantee on the solution outputted but tend to be fast and easy to implement. We will look at how approximation algorithms, which do have some guarantee on the constructed solution, compare in practice to the most used machine learning algorithms both in the quality of the solution (measured using Rand index) and in computation time. 

\section{Literature Review}

Clustering algorithms data back to over 50 years ago, and since then there have been many variations introduced \cite{Jain}. However, clustering algorithms are often formulated as NP-hard problems and are inherently hard to solve \cite{Jain}. When comparing algorithms, we need to consider the stability of the solutions, the computation time, and the proposed objective. 

\subsection{Partitional Clustering}

For partitional clustering, we are often given a number $k$ representing the number of clusters and distances between the data representing a measure of dissimilarity between points, such as Euclidean distance. In addition, many problems require that this distance satisfy the triangle inequality. The objective is then formulated as identifying $k$ cluster centers so as to minimize some function of the distances. 

One popular objective is that of $k$-means in which we want minimize the sum of squared distances from each point to its cluster center \cite{Jain}. The most used algorithm for this problem is Lloyd's $k$-means algorithm which iteratively updates the centers of each cluster (assigning points to the closest center) until the solution becomes stable \cite{Jain}. This algorithm is only guaranteed to reach a local optimum and can be very sensitive to outliers and the initial points chosen \cite{Kanungo}. There also exists a local search algorithm for $k$-means which guarantees to be within a factor of $(9+\varepsilon)$ of the optimal solution that performs well in practice when combined with Lloyd's algorithm \cite{Kanungo}. A relaxation of this problem is fuzzy $c$-means which attempts to minimize the same objective but allows points to belong partially to multiple clusters \cite{ESL}.

Variations on $k$-means attempt to reduce the sensitivity and worst-case behavior. One other objective is to minimize the maximum distance from a point to its cluster center. This problem can be thought of as minimizing the worst case and is called $k$-center clustering.Here, a simply greedy algorithm is guaranteed to come within a factor of $2$ of the optimal solution as long as the given distances satisfy the triangle inequality \cite{Hopcroft}. Furthermore, this is best approximation ratio possible unless P=NP \cite{Shmoys}.  However, $k$-centers does not perform well in practice compared to other methods\cite{Murphy}. This may be do to the fact that it focuses too much on the worst case distance.

An objective function in between $k$-means and $k$-center is two minimize the sum of distances from each point to the median point of its cluster. This problem is called $k$-median or $k$-medoids clustering. Common heuristics for this problem are either greedy, similar to that of $k$-means by repeatedly updating the cluster medians, or are local search algorithms, such as partition around medoids (PAM) \cite{ESL} \cite{Park}. Velmurugan and Santhanam showed that on uniform and normally distributed data the greedy algorithm was empirically computationally more efficient than that of $k$-means \cite{Velmurugan}. 

The $k$-median problem is also a fundamental problem in approximation algorithms. The problem is formulated as chosing $k$ centers from a set $S$ so as to minimize the sum of distances from each point to their closest center. The best known approximation algorithm for this problem guarantees a solution within a factor of 2.592 \cite{Wu}. However, approximation algorithms in this framework are often computationally intensive compared to the local search and greedy algorithms above. Therefore, these algorithms are often not implemented in practice. 

The $k$-means, $k$-median, and $k$-center problems can all be seen as minimizing a supermodular decreasing function subject to a cardinality constraint, a problem related to maximizing a submodular function that is well-studied in approximation algorithms. Furthermore, the approximation algorithms in this area are constructed using local-search or greedy methods and are therefore computationally tractable on large data. In addition, using supermodular functions means our distance measure does not have to satisfy the triangle inequality, which can be useful when considering non-vector data. We will test one of the most recent algorithms for supermodular functions against the greedily updating $k$-means and $k$-median algorithms. While there exist many other methods for clustering such as affinity propogation, spectral clustering, and mixture models \cite{Murphy}, we will restrict the comparison of our proposed algorithm to the algorithms above which are widely used in practice and can relate in objective function.

\subsection{Hierarchical Clustering}


\section{Partitional Clustering}
Given a set $X$, a function $f:2^{X} \mapsto \mathbb{R}$ is supermodular, if for all $T \subseteq S \subseteq X$ and $j \notin S$, 
\[ f(T \cup \{j\}) - f(T) \leq f(S \cup \{j\}) - f(S). \]
Furthermore, $f$ is non-increasing if for all $T \subseteq S \subseteq X$, $f(S) \leq f(T)$. 

Suppose we have a set of data $X$ of which we want to choose a subset $S$ such that $|S| \leq k$ so as to minimize 
\[ f(S) = \sum_{i \in X} \min_{j \in S} d(i,j) ,\]
where we can think of $d$ as some measure of dissimilarity (for example squared distance or absolute distance). For completeness, we define $f(\emptyset) = \min_{j \in S} f(\{j\})$. 
Note that this generalizes the $k$-median problem and can capture a variant of $k$-means where are restricted to choosing data points as centers.

It is clear that $f$ is non-increasing since for all $T \subseteq S \subseteq X$ and $i \in X$, $\min_{j \in S} d(i,j) \leq \min_{j \in T} d(i,j)$. Furthermore, for $j \notin S$, adding $j$ to $T$ will decrease the sum of the distances more than adding $j$ to $S$ so $f$ is supermodular. In general, there is no known approximation algorithm for minimizing a non-increasing supermodular function subject to a cardinality constraint. However, if we can place some properties on the ``steepnesss'' of $f$ then there is a known optimal approximation algorithm \cite{Sviridenko}.

As in \cite{Sviridenko}, we define the \emph{total curvature} $c$ of a non-increasing supermodular function $f$ as 
\[ c = 1 - \min_{j \in X}  \frac{ f(X) - f(X- \{j\})}{ f(\{j\}) - f(\emptyset)}.\]
Then, for every $\varepsilon >0$ and $c \in [0,1)$, there exists a local search algorithm that produces a set $S \subseteq X$ such that $|S| \leq k$ in polynomial time such that 
\[ f(S) \leq \left( 1 + \frac{c}{1-c} \cdot e^{-1} + \frac{1}{1-c} \cdot O(\varepsilon) \right) f(O) \]
for all $O \subseteq X$ such that $|O| \leq k$, with high probability [Sviridenko]. Note that $c$ will vary depending on our dataset so while in some cases we may have a very good approximation ratio it can worsen as our distances become non-smooth (for example distances that do not form a metric). 

This algorithm first defines linear and submodular functions on $X$:
\begin{align*}
l(A) &= \sum_{j \in A} f(\{j\}) - f(\emptyset) \\
g(A) & = - l(A) - f(X \setminus A).
\end{align*}
Furthermore, we let $\hat{v}_g = \max_{j \in X} g(j)$, $\hat{v}_l = \max_{j \in X} |l(j)|$, and $\hat{v} = \max(\hat{v}_g , \hat{v}_l)$. 

Given a set $A \subseteq X$, we consider a two-step random sampling. We first draw a value $p \in [0,1]$ using the density function $\frac{{e}^p}{e-1}$. Then, we choose a subset $B \in A$ by selecting each $x \in A$ to be in $B$ independently with probability $p$. The expected value of $g(B)$ is given by
\[  h(A) = \sum_{B \subseteq A} g(B) \cdot \int_{0}^1 \frac{e^p}{e-1} \cdot p^{|B|-1} (1-p)^{|A|-|B|} dp. \]
Then, we define our potential function
\[ \psi(A) = (1-e^{-1}) h(A) + l(A). \]

We are now ready to describe how the algorithm works. The algorithm sets $\delta = \frac{\varepsilon}{n}$ to be the maximum step size and starts with a set $S$ of size $k$. Then, while there exists $a \in S$ and $b \in X \setminus S$ such that 
\[ \psi(S -a +b) \geq \psi(S) + \delta, \]
the algorithm removes $a$ from $S$ and adds $b$. 

Note that $\psi$ requires summing an exponential number of terms. However, we estimate $h(A)$ (and therefore $\psi(A)$) by randomly sampling $g(B)$ in the above fashion. If $\hat{\psi}(A)$ is an estimate of $\psi(A)$ sampled from $\Omega (\varepsilon^{-2} n^4 \ln^4 n \ln M)$ samples, then 
\[ Pr[ | \hat{\psi}(A) - \psi(A) | \geq \delta ] = O(M^{-1}). \]
By setting $M$ large enough (polynomially large), the algorithm will converge in polynomial time and will return a solution satisying the above guarantee with high probability \cite{Sviridenko}. 
 
\subsection{Implmentation}

\subsection{Results}

\section{Hierarchical Clustering}

\
\label{submission}

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\bibliography{projectBib}
\bibliographystyle{icml2014}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
